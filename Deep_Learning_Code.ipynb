{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gJzy6OzPQYdW",
        "oCK_j2fOVG5Q",
        "A2BsUn9SP6FO",
        "Tv7pRw4YQGLV",
        "RapfARWQQBfE",
        "SWD6V7fRapW1",
        "c34_R4Nbau5y",
        "tDEk4GPQwX3V",
        "B_MeZrKAQ-nA",
        "-sa3lDPMELTM"
      ],
      "authorship_tag": "ABX9TyPpcvP4aC9UQbW3gX36eDZd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramirog034/TQx/blob/main/Deep_Learning_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone GitHub Repo"
      ],
      "metadata": {
        "id": "gJzy6OzPQYdW"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31c1211e"
      },
      "source": [
        "# clone the repo\n",
        "!git clone https://github.com/QuIIL/TQx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "oCK_j2fOVG5Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4294ab0"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f58a6df0"
      },
      "source": [
        "# ensure a shortcut to the shared Bladder folder exists on you Drive\n",
        "# copy the files into the TQx Bladder folder\n",
        "!cp '/content/drive/MyDrive/Bladder/all_img_features_sorted.pkl' '/content/TQx/results/Bladder'\n",
        "!cp '/content/drive/MyDrive/Bladder/all_img_features_sorted_test.pkl' '/content/TQx/results/Bladder'\n",
        "!cp '/content/drive/MyDrive/Bladder/all_img_features_sorted_train.pkl' '/content/TQx/results/Bladder'\n",
        "!cp '/content/drive/MyDrive/Bladder/all_img_features_sorted_valid.pkl' '/content/TQx/results/Bladder'\n",
        "!cp '/content/drive/MyDrive/Bladder/img_info_test.txt' '/content/TQx/results/Bladder'\n",
        "!cp '/content/drive/MyDrive/Bladder/img_info_train.txt' '/content/TQx/results/Bladder'\n",
        "!cp '/content/drive/MyDrive/Bladder/img_info_valid.txt' '/content/TQx/results/Bladder'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "A2BsUn9SP6FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import csv\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, cohen_kappa_score, confusion_matrix\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "from sklearn.cluster import KMeans\n",
        "from matplotlib import cm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from PIL import Image\n",
        "from IPython import display"
      ],
      "metadata": {
        "id": "lZTUaRy6P4o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Softmax Function"
      ],
      "metadata": {
        "id": "Tv7pRw4YQGLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))  # Subtracting np.max(x) for numerical stability\n",
        "    return 100* e_x / e_x.sum(axis=0)"
      ],
      "metadata": {
        "id": "QOCebD9VQHw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mappings"
      ],
      "metadata": {
        "id": "RapfARWQQBfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def config_for_dataset(dataset):\n",
        "    if dataset in ['colon-1', 'Colon']:\n",
        "        mapping = {'moderately differentiated cancer': 'MD',\n",
        "                    'poorly differentiated cancer': 'PD',\n",
        "                    'benign': 'BN',\n",
        "                    'well differentiated cancer': 'WD'}\n",
        "        mapping_2 = {'moderately differentiated cancer': 2,\n",
        "                    'poorly differentiated cancer': 3,\n",
        "                    'benign': 0,\n",
        "                    'well differentiated cancer': 1}\n",
        "        mapping_3 = {2: 'MD',\n",
        "                    3: 'PD',\n",
        "                    0: 'BN',\n",
        "                    1: 'WD'}\n",
        "        custom_order=['BN', 'WD', 'MD', 'PD']\n",
        "    elif dataset in ['luad', 'WSSS4LUAD']:\n",
        "        mapping = {'tumor': 'TUM',\n",
        "                    'normal': 'NOR'}\n",
        "        mapping_2 = {'tumor': 1,\n",
        "                    'normal': 0}\n",
        "        mapping_3 = {1:'TUM',\n",
        "                    0:'NOR'}\n",
        "        custom_order=['NOR', 'TUM']\n",
        "    elif dataset in ['bach', 'BACH']:\n",
        "        mapping = {'invasive carcinoma': 'IVS',\n",
        "                   'in situ carcinoma': 'SITU',\n",
        "                    'benign': 'BN',\n",
        "                    'normal': 'NOR'}\n",
        "        mapping_2 = {'invasive carcinoma': 3,\n",
        "                   'in situ carcinoma': 2,\n",
        "                    'benign': 1,\n",
        "                    'normal': 0}\n",
        "        mapping_3 = {1:'BN',\n",
        "                     2: 'SITU',\n",
        "                     3: 'IVS',\n",
        "                    0:'NOR'}\n",
        "        custom_order=['NOR', 'BN', 'SITU', 'IVS']\n",
        "    elif dataset in ['bladder', 'Bladder']:\n",
        "        mapping = {'high grade cancer': 'HIGH',\n",
        "                   'low grade cancer': 'LOW',\n",
        "                    'normal': 'NOR'}\n",
        "        mapping_2 = {'high grade cancer': 2,\n",
        "                     'low grade cancer': 1,\n",
        "                    'normal': 0}\n",
        "        mapping_3 = {2: 'HIGH',\n",
        "                    1:'LOW',\n",
        "                    0:'NOR'}\n",
        "        custom_order=['NOR', 'LOW', 'HIGH']\n",
        "    return mapping, mapping_2, mapping_3, custom_order"
      ],
      "metadata": {
        "id": "i6jy1mW7QAjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering"
      ],
      "metadata": {
        "id": "SWD6V7fRapW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd TQx/"
      ],
      "metadata": {
        "id": "Lg7irt8VjDW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clustering(args, features, img_label, path, postfix):\n",
        "    features = torch.tensor(features).numpy()\n",
        "\n",
        "    y = img_label\n",
        "    mapping = config_for_dataset(args.dataset)[1]\n",
        "    y = [mapping[i] for i in y]\n",
        "\n",
        "    label_mapping = config_for_dataset(args.dataset)[2]\n",
        "    cluster_labels_list = []\n",
        "\n",
        "    kmeans = KMeans(n_clusters=len(label_mapping), init='k-means++', n_init='auto')\n",
        "    cluster_labels = kmeans.fit_predict(features)\n",
        "    cluster_labels_list.append(cluster_labels)\n",
        "\n",
        "    return cluster_labels_list\n",
        "\n",
        "def scale_list(original_list, scale_factor):\n",
        "    scaled_list = []\n",
        "    for item in original_list:\n",
        "        scaled_list.extend([item] * scale_factor)\n",
        "    return scaled_list\n",
        "\n",
        "def save_string_to_file(string, file_path):\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(string)\n",
        "\n",
        "def analyze_clustering(args, raw_df, path, postfix):\n",
        "    raw_df = raw_df[raw_df['match_rank'] < args.top_analyze]\n",
        "    df = raw_df[raw_df['match_rank']==1]\n",
        "    df['label'] = df['label'].map(args.mapping)\n",
        "    k = df['label'].nunique()\n",
        "    labels = df['label'].unique()\n",
        "    custom_order = args.custom_order\n",
        "    custom_values = df['label'].value_counts()\n",
        "\n",
        "    fig, axes = plt.subplots(1, k, figsize=(3*k, 2.3))\n",
        "    s = ''\n",
        "\n",
        "    if args.dataset in ['colon-1', 'Colon']:\n",
        "        display = 'Colon'\n",
        "    elif args.dataset in ['bladder', 'Bladder']:\n",
        "        display = 'Bladder'\n",
        "    elif args.dataset in ['bach', 'BACH']:\n",
        "        display = 'BACH'\n",
        "    elif args.dataset in ['luad', 'WSSS4LUAD']:\n",
        "        display = 'WSSS4LUAD'\n",
        "\n",
        "    green = '#55a868'\n",
        "    orange = '#dd8452'\n",
        "    red = '#c44f52'\n",
        "    blue = '#4c72b0'\n",
        "\n",
        "    colors = [green, blue, orange, red]\n",
        "\n",
        "    for cluster in range(k):\n",
        "        entities_counts = raw_df[raw_df[f'k = {k}'] == cluster]['entity'].value_counts().head(20).to_string()\n",
        "        s += entities_counts + '\\n'\n",
        "        value_counts = df[df[f'k = {k}'] == cluster]['label'].value_counts().reindex(custom_order)\n",
        "\n",
        "        value_counts = value_counts / df[df[f'k = {k}'] == cluster]['label'].value_counts().sum()\n",
        "        # value_counts = value_counts / value_counts.index.map(custom_values)\n",
        "        ax = axes[cluster]\n",
        "\n",
        "        df2 = value_counts.to_frame().reset_index()\n",
        "        df2.columns = ['Category', 'Count']\n",
        "\n",
        "        sns.barplot(ax=ax,\n",
        "                    x='Category',\n",
        "                    y='Count',\n",
        "                    data=df2,\n",
        "                    palette=colors,\n",
        "                    width=1/2\n",
        "                    )\n",
        "\n",
        "        ax.set_title(f'Cluster {cluster+1}')\n",
        "        ax.title.set_size(17)\n",
        "        ax.tick_params(axis='x', labelsize=13)\n",
        "        ax.set_xlabel('')\n",
        "        ax.set_ylabel('')\n",
        "\n",
        "    # Function to format tick labels\n",
        "    def format_ticks(x, pos):\n",
        "        return \"{:.1f}\".format(x)\n",
        "\n",
        "    # Apply formatting to both axes\n",
        "    # plt.gca().xaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
        "    for ax in axes.flat:\n",
        "        ax.yaxis.set_major_formatter(FuncFormatter(format_ticks))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{path}/k-{k}{postfix}-within.png', dpi=600)\n",
        "    file_path = f'{path}/common_terms-within.txt'\n",
        "    save_string_to_file(s, file_path)\n",
        "\n",
        "def calculate_sim(args):\n",
        "    ###########################################################################\n",
        "    # added the if else block to locally define display\n",
        "    # Corrected logic to ensure 'display' is always assigned based on args.dataset\n",
        "    if args.dataset in ['colon-1', 'Colon']:\n",
        "        display = 'Colon'\n",
        "    elif args.dataset in ['bladder', 'Bladder']:\n",
        "        display = 'Bladder'\n",
        "    elif args.dataset in ['bach', 'BACH']:\n",
        "        display = 'BACH'\n",
        "    elif args.dataset in ['luad', 'WSSS4LUAD']:\n",
        "        display = 'WSSS4LUAD'\n",
        "\n",
        "    print(f'Clustering {args.dataset}')\n",
        "    postfix = args.postfix\n",
        "\n",
        "    ###########################################################################\n",
        "    # changed args.dataset to display\n",
        "    entity_name_path = 'entity.csv'\n",
        "    entity_feature_path = 'entity_ALL_FEATURES.pkl'\n",
        "    image_feature_path = f'results/{display}/all_img_features_sorted{postfix}.pkl'\n",
        "    image_info_path = f'results/{display}/img_info{postfix}.txt'\n",
        "\n",
        "    filters = args.filters\n",
        "    path = f'results/{display}/{filters}_{args.top_freq_words}_{args.top_freq_features_to_combine}'\n",
        "\n",
        "    entity_name = pd.read_csv(entity_name_path)\n",
        "    with open(entity_feature_path, 'rb') as f:\n",
        "        entity_feature = pickle.load(f)\n",
        "    with open(image_feature_path, 'rb') as f:\n",
        "        image_feature = pickle.load(f)\n",
        "    with open(image_info_path, 'r') as file:\n",
        "        img_files = file.readlines()\n",
        "\n",
        "    # FILTER 1 using semantic name\n",
        "    if len(args.filter_semantic_name) != 0:\n",
        "        mask = entity_name['semantic_name'].isin(args.filter_semantic_name)\n",
        "        indices = entity_name.index[mask]\n",
        "        entity_name = entity_name.iloc[indices].reset_index(drop=True)\n",
        "        entity_feature = torch.index_select(entity_feature, 0, torch.tensor(indices))\n",
        "\n",
        "    norm_entity_feature = entity_feature / entity_feature.norm(dim=-1, keepdim=True)\n",
        "    norm_image_feature = image_feature/ image_feature.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    sim = norm_image_feature @ norm_entity_feature.T\n",
        "\n",
        "    # FILTER 2 using top freq words\n",
        "    sorted_indices = torch.argsort(sim, dim=1, descending=True)\n",
        "    ranks = torch.zeros_like(sorted_indices, dtype=torch.float)\n",
        "    for i in range(sim.shape[0]):  # Iterate over rows\n",
        "        ranks[i, sorted_indices[i, :]] = torch.arange(sim.shape[1], dtype=torch.float) + 1\n",
        "    ranks = torch.sum(ranks, dim=0)/sim.shape[0]  # avg rank of each word\n",
        "    entity_name['avg_ranking'] = ranks.numpy()\n",
        "    entity_name = entity_name.nsmallest(args.top_freq_words, 'avg_ranking')\n",
        "    indices = entity_name.index\n",
        "    entity_feature = torch.index_select(entity_feature, 0, torch.tensor(indices))\n",
        "    entity_name = entity_name.reset_index(drop=True)\n",
        "\n",
        "    norm_entity_feature = entity_feature / entity_feature.norm(dim=-1, keepdim=True)\n",
        "    sim = norm_image_feature @ norm_entity_feature.T\n",
        "\n",
        "    image_text_representation = torch.matmul(sim, entity_feature)/torch.sum(sim,dim=1)[:,None]\n",
        "\n",
        "    if not os.path.exists(f'{path}'):\n",
        "        os.makedirs(f'{path}')\n",
        "    with open(f'{path}/image_text_representation{postfix}.pkl', 'wb') as file:\n",
        "        pickle.dump(image_text_representation, file)\n",
        "\n",
        "    top_values, top_indices = torch.topk(sim, k=args.top_freq_features_to_combine, dim=1) # shape (num_img, top_entities)\n",
        "    m = torch.nn.Softmax(dim=1)\n",
        "    softmax_output = m(sim)\n",
        "    top_values_2, top_indices_2 = torch.topk(softmax_output, k=args.top_freq_features_to_combine, dim=1)\n",
        "\n",
        "    if args.dataset == 'gastric':\n",
        "        split = '.jpg,'\n",
        "    elif args.dataset in ['luad', 'WSSS4LUAD']:\n",
        "        split = '.png,'\n",
        "    else:\n",
        "        split = ','\n",
        "\n",
        "    img_path = [file.split(split)[0] for file in img_files]\n",
        "    img_path = scale_list(img_path, args.top_freq_features_to_combine)\n",
        "\n",
        "    raw_label = [file.split(split)[1][:-2] for file in img_files]\n",
        "    label = scale_list(raw_label, args.top_freq_features_to_combine)\n",
        "\n",
        "    match_rank = [i for i in range(args.top_freq_features_to_combine)]\n",
        "    match_rank = match_rank * len(img_files)\n",
        "\n",
        "    top_idx = pd.Series(top_indices.view(-1))\n",
        "    entity = top_idx.map(entity_name['entity_name'].to_dict())\n",
        "    entity_semantic = top_idx.map(entity_name['semantic_name'].to_dict())\n",
        "\n",
        "    probability = top_values.view(-1)\n",
        "    cosine_sim = top_values_2.view(-1)\n",
        "\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    cluster_labels_list = clustering(args, image_text_representation, raw_label, path, postfix)[0]\n",
        "\n",
        "    columns = [\"image_path\", \"label\", \"match_rank\", \"entity\", \"entity_semantic\", \"probability\", 'cosine_sim']\n",
        "    df = pd.DataFrame(columns=columns)\n",
        "    df['image_path'] = img_path\n",
        "    df['match_rank'] = match_rank\n",
        "    df['label'] = label\n",
        "    df['entity'] = entity\n",
        "    df['entity_semantic'] = entity_semantic\n",
        "    df['probability'] = probability\n",
        "    df['cosine_sim'] = cosine_sim\n",
        "    filters = ('_').join(args.filter_semantic_name).replace(' ','_').replace(';','')\n",
        "    df[f\"k = {df['label'].nunique()}\"] = np.repeat(cluster_labels_list, args.top_freq_features_to_combine)\n",
        "    df_csv = df[df['match_rank'] < args.top_words_in_csv]\n",
        "    df_csv.to_csv(f'{path}/top_{args.top_words_in_csv}{args.postfix}.csv', index=False)\n",
        "\n",
        "    analyze_clustering(args, df, path, postfix)\n",
        "    return cluster_labels_list\n",
        "\n",
        "def measure_metrics(dataset, pred, label):\n",
        "    if dataset in ['colon-1', 'colon-2', 'prostate-1', 'prostate-2', 'prostate-3',\n",
        "                   'gastric','kidney','liver','bladder','bach','panda']:\n",
        "        acc = accuracy_score(label, pred)\n",
        "\n",
        "        excluded_class = 0\n",
        "\n",
        "        temp_1 = []\n",
        "        temp_2 = []\n",
        "        for i, l in enumerate(label):\n",
        "            if l != excluded_class:\n",
        "                temp_1.append(l)\n",
        "                temp_2.append(pred[i])\n",
        "        acc_grading = accuracy_score(temp_1, temp_2)\n",
        "        f1 = f1_score(label, pred, average='macro')\n",
        "        kappa = cohen_kappa_score(label, pred, labels=np.arange(len(np.unique(pred))), weights='quadratic')\n",
        "        cm = confusion_matrix(label, pred, labels=[0,1,2,3])\n",
        "        result = (acc, acc_grading, f1, kappa, cm)\n",
        "\n",
        "    else:\n",
        "        acc = accuracy_score(label, pred)\n",
        "        f1 = f1_score(label, pred, average='macro')\n",
        "        rec = recall_score(label, pred, average='macro')\n",
        "        pre = precision_score(label, pred, average='macro')\n",
        "        cm = confusion_matrix(label, pred, labels=[0,1,2,3])\n",
        "        result = (acc, pre, f1, rec, cm)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "b_nk4lbdaqsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "c34_R4Nbau5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(args):\n",
        "    print(args.test, args.feature_type, args.filter_semantic_name)\n",
        "\n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(MLP, self).__init__()\n",
        "            self.fc1 = nn.Linear(512, 2048)\n",
        "            self.relu = nn.ReLU()\n",
        "            self.fc2 = nn.Linear(2048, output_size)\n",
        "            self.bnorm = nn.BatchNorm1d(2048)\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.fc1(x)\n",
        "            x = self.bnorm(x)\n",
        "            x = self.relu(x)\n",
        "            x = self.fc2(x)\n",
        "            return x\n",
        "\n",
        "    mapping = config_for_dataset(args.dataset)[1]\n",
        "\n",
        "    filters = args.filters\n",
        "    path1 = f'results/{args.dataset}/{filters}_{args.top_freq_words}_{args.top_freq_features_to_combine}'\n",
        "    path2 = f'results/{args.test}/{filters}_{args.top_freq_words}_{args.top_freq_features_to_combine}'\n",
        "\n",
        "    if args.feature_type == 'text':\n",
        "        train_feature_path = f'{path1}/image_text_representation_train.pkl'\n",
        "        valid_feature_path = f'{path1}/image_text_representation_valid.pkl'\n",
        "        test_feature_path = f'{path2}/image_text_representation_test.pkl'\n",
        "    elif args.feature_type == 'image':\n",
        "        train_feature_path = f'results/{args.dataset}/all_img_features_sorted_train.pkl'\n",
        "        valid_feature_path = f'results/{args.dataset}/all_img_features_sorted_valid.pkl'\n",
        "        test_feature_path = f'results/{args.test}/all_img_features_sorted_test.pkl'\n",
        "\n",
        "    if args.test in ['colon-2', 'prostate-2', 'prostate-3', 'k16']:\n",
        "        test_feature_path = test_feature_path.replace('_test', '')\n",
        "\n",
        "    with open(train_feature_path, 'rb') as file:\n",
        "        train_feature = pickle.load(file)\n",
        "    image_info_path = f'results/{args.dataset}/img_info_train.txt'\n",
        "    with open(image_info_path, 'r') as file:\n",
        "        train_img_files = file.readlines()\n",
        "    if args.dataset == 'gastric':\n",
        "        split = '.jpg,'\n",
        "    elif args.dataset == 'luad' or args.dataset == 'WSSS4LUAD':\n",
        "        split = '.png,'\n",
        "    else:\n",
        "        split = ','\n",
        "    train_label = [file.split(split)[1][:-2] for file in train_img_files]\n",
        "    train_label_list = [mapping[label] for label in train_label]\n",
        "    train_label = torch.tensor(train_label_list)\n",
        "\n",
        "    with open(valid_feature_path, 'rb') as file:\n",
        "        valid_feature = pickle.load(file)\n",
        "    image_info_path = f'results/{args.dataset}/img_info_valid.txt'\n",
        "    with open(image_info_path, 'r') as file:\n",
        "        valid_img_files = file.readlines()\n",
        "    valid_label = [file.split(split)[1][:-2] for file in valid_img_files]\n",
        "    valid_label = [mapping[label] for label in valid_label]\n",
        "\n",
        "    with open(test_feature_path, 'rb') as file:\n",
        "        test_feature = pickle.load(file)\n",
        "    image_info_path = f'results/{args.test}/img_info_test.txt'\n",
        "    with open(image_info_path, 'r') as file:\n",
        "        test_img_files = file.readlines()\n",
        "    test_label = [file.split(split)[1][:-2] for file in test_img_files]\n",
        "    test_label = [mapping[label] for label in test_label]\n",
        "\n",
        "    # Define hyperparameters\n",
        "    input_size = 512  # Size of the input features\n",
        "    hidden_size = 4096  # Size of the hidden layer\n",
        "    output_size = len(set(test_label))\n",
        "\n",
        "    # Initialize model, loss function, and optimizer\n",
        "    device = torch.device('cuda:2')\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 300\n",
        "\n",
        "    temp = None\n",
        "    metrics_list = [\n",
        "        [],[],[],[]\n",
        "    ]\n",
        "    for seed in args.seed:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        model = MLP(input_size, hidden_size, output_size).to(device)\n",
        "        model.train()\n",
        "        optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
        "        max_f1 = 0\n",
        "        for _ in range(num_epochs):\n",
        "            optimizer.zero_grad()\n",
        "            train_pred = model(train_feature.to(device))\n",
        "            loss = criterion(train_pred, train_label.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # print(torch.argmax(train_pred[0]))\n",
        "            train_pred = torch.argmax(train_pred, dim=1).tolist()\n",
        "\n",
        "            val_predictions = model(valid_feature.to(device))\n",
        "            val_predictions = torch.argmax(val_predictions, dim=1).tolist()\n",
        "\n",
        "            test_predictions = model(test_feature.to(device))\n",
        "            test_predictions = torch.argmax(test_predictions, dim=1).tolist()\n",
        "\n",
        "            metrics = measure_metrics(args.dataset, val_predictions, valid_label)\n",
        "            if metrics[2] > max_f1:\n",
        "                max_f1 = metrics[2]\n",
        "                temp = measure_metrics(args.dataset, test_predictions, test_label)[:-1]\n",
        "        for i in range(4):\n",
        "            metrics_list[i].append(temp[i])\n",
        "    new_row = [args.test, filters, args.feature_type,\n",
        "               f\"{np.mean(metrics_list[0]):.4f}+{np.std(metrics_list[0]):.4f}\",\n",
        "               f\"{np.mean(metrics_list[1]):.4f}+{np.std(metrics_list[1]):.4f}\",\n",
        "               f\"{np.mean(metrics_list[2]):.4f}+{np.std(metrics_list[2]):.4f}\",\n",
        "               f\"{np.mean(metrics_list[3]):.4f}+{np.std(metrics_list[3]):.4f}\"]\n",
        "\n",
        "    # File path of the existing CSV file\n",
        "    file_path = 'classification.csv'\n",
        "\n",
        "    # Open the CSV file in append mode\n",
        "    with open(file_path, 'a', newline='') as file:\n",
        "        # Create a CSV writer object\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write the new row to the CSV file\n",
        "        writer.writerow(new_row)\n",
        "\n",
        "\n",
        "    print(temp)"
      ],
      "metadata": {
        "id": "xGFBL83-wQpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cluster Plotting"
      ],
      "metadata": {
        "id": "tDEk4GPQwX3V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eD8iSacPwe6"
      },
      "outputs": [],
      "source": [
        "def plotting_data_split(cluster_labels, args):\n",
        "    # create t-SNE object to plot 2D graph, random state, disable progress logging\n",
        "    tsne = TSNE(n_components=2,\n",
        "                random_state=42, verbose=0)\n",
        "\n",
        "    # construct path to pickled embeddings for training set\n",
        "    train_feature_path = f'results/{args.dataset}/Neoplastic_Process_1000_1000/image_text_representation_train.pkl'\n",
        "    # open in binary read mode and unpickle into train_feature tensor of 512 dims\n",
        "    with open(train_feature_path, 'rb') as f:\n",
        "        train_feature = pickle.load(f)\n",
        "    # run t-SNE on it using fit_transform, returning 2D numpy array (samples, 2)\n",
        "    tsne_train = tsne.fit_transform(train_feature)\n",
        "\n",
        "    # same but for valid\n",
        "    valid_feature_path = f'results/{args.dataset}/Neoplastic_Process_1000_1000/image_text_representation_valid.pkl'\n",
        "    with open(valid_feature_path, 'rb') as f:\n",
        "        valid_feature = pickle.load(f)\n",
        "    tsne_valid = tsne.fit_transform(valid_feature)\n",
        "\n",
        "    # same but for test\n",
        "    test_feature_path = f'results/{args.dataset}/Neoplastic_Process_1000_1000/image_text_representation_test.pkl'\n",
        "    with open(test_feature_path, 'rb') as f:\n",
        "        test_feature = pickle.load(f)\n",
        "    tsne_test = tsne.fit_transform(test_feature)\n",
        "\n",
        "    # txt contain image filenames and their class labels separated by a delimiter\n",
        "    img_info_train = f'results/{args.dataset}/img_info_train.txt'\n",
        "    img_info_valid = f'results/{args.dataset}/img_info_valid.txt'\n",
        "    img_info_test = f'results/{args.dataset}/img_info_test.txt'\n",
        "\n",
        "    # choose proper delimiter corresponding to dataset\n",
        "    if args.dataset == 'gastric':\n",
        "        split = '.jpg,'\n",
        "    elif args.dataset == 'luad' or args.dataset == 'WSSS4LUAD':\n",
        "        split = '.png,'\n",
        "    else:\n",
        "        split = ','\n",
        "\n",
        "    # select proper mappings based on dataset\n",
        "    dataset_configs = config_for_dataset(args.dataset)\n",
        "\n",
        "    # [0] is the string to acronym\n",
        "    # [1] is string to int\n",
        "    # [2] is int to acronym or int to str\n",
        "    # [3] is the acronyms only, or custom order labels\n",
        "    str_to_int_map = dataset_configs[1]\n",
        "    int_to_str_map = dataset_configs[2]\n",
        "    custom_order_str_labels = dataset_configs[3]\n",
        "\n",
        "    # read all the lines from the train txt file, which contains ground truth labels labels\n",
        "    with open(img_info_train, 'r') as file:\n",
        "        img_files_train = file.readlines()\n",
        "    # for each line, split at delimiter, gives list like [\"/img_11769_1\", \"benign.\\n\"] getting rid of .jpg,\n",
        "    # [1] means second token, the label and :-2 removes trailing characters. could use .strip()\n",
        "    # convert it to its corresponding int\n",
        "    label_train_str = [file.split(split)[1][:-2] for file in img_files_train]\n",
        "    label_train_int = [str_to_int_map[label] for label in label_train_str]\n",
        "\n",
        "    # same for valid\n",
        "    with open(img_info_valid, 'r') as file:\n",
        "        img_files_valid = file.readlines()\n",
        "    label_valid_str = [file.split(split)[1][:-2] for file in img_files_valid]\n",
        "    label_valid_int = [str_to_int_map[label] for label in label_valid_str]\n",
        "\n",
        "    # same for test\n",
        "    with open(img_info_test, 'r') as file:\n",
        "        img_files_test = file.readlines()\n",
        "    label_test_str = [file.split(split)[1][:-2] for file in img_files_test]\n",
        "    label_test_int = [str_to_int_map[label] for label in label_test_str]\n",
        "\n",
        "    # create 10in by 10in single subplot for visualization\n",
        "    # subplots creates a Figure object which is overall container that holds canvas\n",
        "    # may also create one or more Axes objects, the actual plotting areas/coordinate system inside the figure\n",
        "    # 1, 1 means 1 row, 1 col so just a single Axes obj\n",
        "    fig, axs = plt.subplots(1, 1, figsize=(10, 10))\n",
        "\n",
        "    # color scheme\n",
        "    green = '#55a868'\n",
        "    orange = '#dd8452'\n",
        "    red = '#c44f52'\n",
        "    blue = '#4c72b0'\n",
        "    colors = [green, orange, blue, red]\n",
        "    i = 0\n",
        "\n",
        "    # concatenate all integer labels from all splits\n",
        "    all_labels_int_combined = label_train_int + label_valid_int + label_test_int\n",
        "\n",
        "    # above uses ground truth labels. use cluster labels if not empty\n",
        "    if len(cluster_labels) != 0:\n",
        "        all_labels_int_combined = np.concatenate((cluster_labels[\"_train\"], cluster_labels[\"_valid\"], cluster_labels[\"_test\"]))\n",
        "\n",
        "    # set for unique ids. list so we can iterate in specific order later, sorting for predictability\n",
        "    unique_class_ids = sorted(list(set(all_labels_int_combined)))\n",
        "\n",
        "    # a handle is the thing next to the label. for us its the dots in the legend, one per class\n",
        "    legend_handles = []\n",
        "    legend_labels = []\n",
        "    plotted_labels_for_legend = set() # To ensure each class label appears only once in the legend\n",
        "\n",
        "    for class_int_label in unique_class_ids:\n",
        "        # Get the string representation for the legend\n",
        "        class_str_label = int_to_str_map[class_int_label]\n",
        "\n",
        "        # Iterate through the data splits one at a time so that feature_tsne is current split's t-SNE data,\n",
        "        # and labels_int is mathing list of labels\n",
        "        # zip takes two+ iterables and combines them element-wise into pairs\n",
        "        # enumerate adds index counter starting with 0 default\n",
        "        # so we get 3 iterations. first one uses (0, (tsne_train, label_train_int))\n",
        "        for split_type_idx, (feature_tsne, labels_int) in enumerate(zip(\n",
        "            [tsne_train, tsne_valid, tsne_test],\n",
        "            [label_train_int, label_valid_int, label_test_int]\n",
        "        )):\n",
        "            # create numpy boolean mask selecting only the rows (points) belonging to current class\n",
        "            current_class_features = feature_tsne[np.array(labels_int) == class_int_label]\n",
        "\n",
        "            # Determine color from the original cycling list\n",
        "            current_color = colors[i]\n",
        "\n",
        "            # Only add label for legend once per class\n",
        "            if class_str_label not in plotted_labels_for_legend:\n",
        "                # draw the points\n",
        "                # Store a proxy artist for the legend, using the first color encountered for this class\n",
        "                # using : for all rows (all points) and 0 is x, 1 is y\n",
        "                sc = axs.scatter(current_class_features[:, 0], current_class_features[:, 1],\n",
        "                                 c=current_color, alpha=0.7, label=class_str_label)\n",
        "                legend_handles.append(sc)\n",
        "                legend_labels.append(class_str_label)\n",
        "                plotted_labels_for_legend.add(class_str_label)\n",
        "            else:\n",
        "                # For subsequent plots of the same class (different splits), don't add a label\n",
        "                axs.scatter(current_class_features[:, 0], current_class_features[:, 1],\n",
        "                                 c=current_color, alpha=0.7)\n",
        "        # update i. since we do this in the outer loop, color changes per class\n",
        "        i += 1\n",
        "\n",
        "    # set title of the image based on which labels we are using\n",
        "    if len(cluster_labels) != 0:\n",
        "      axs.set_title(f't-SNE of Image-Text Representations for {args.dataset} cluster labels', fontsize = 24)\n",
        "    else:\n",
        "      axs.set_title(f't-SNE of Image-Text Representations for {args.dataset} true Labels', fontsize = 24)\n",
        "\n",
        "    # Use the collected handles and labels to create the legend\n",
        "    axs.legend(handles=legend_handles, labels=legend_labels, title='Classes', loc='best')\n",
        "    # dashed grid lines for visiblity\n",
        "    axs.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # save to png and display plot\n",
        "    if len(cluster_labels) != 0:\n",
        "      plt.savefig(f'tsne_plot_{args.dataset}_cluster_labels.png', dpi=600)\n",
        "    else:\n",
        "      plt.savefig(f'tsne_plot_{args.dataset}_true_labels.png', dpi=600)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Function"
      ],
      "metadata": {
        "id": "B_MeZrKAQ-nA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(dataset, postfix):\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--dataset', default=dataset)\n",
        "    parser.add_argument('--test', choices=['colon-1','bach','bladder', 'luad'], default='')\n",
        "    parser.add_argument('--feature_type', type=str, default='text')\n",
        "    parser.add_argument('--seed', default=list(range(50)))\n",
        "    parser.add_argument('--plot', choices=['tsne','umap'], default='tsne')\n",
        "    parser.add_argument('--filter_semantic_name', nargs='+', default=['Neoplastic Process;'])\n",
        "    parser.add_argument('--filters',default='Neoplastic_Process')\n",
        "    parser.add_argument('--postfix', default=postfix)\n",
        "    parser.add_argument('--text_split_range', type=int, default=1)\n",
        "    parser.add_argument('--img_split_range', type=int, default=8192)\n",
        "    parser.add_argument('--device', type=int, default=2)\n",
        "    parser.add_argument('--draw', type=bool, default=False)\n",
        "    parser.add_argument('--num_workers', type=int, default=10)\n",
        "\n",
        "    parser.add_argument('--top_words_in_csv', type=int, default=20)\n",
        "    parser.add_argument('--top_analyze', default=50)\n",
        "    parser.add_argument('--top_freq_features_to_combine', type=int, default=1000)\n",
        "    parser.add_argument('--top_freq_words', type=int, default=1000)\n",
        "    parser.add_argument('--n_clusters', default=[2,3,4,5,6,7])\n",
        "    parser.add_argument('--mode', default='0')\n",
        "\n",
        "    parser.add_argument('--mapping', default=\n",
        "                        {'moderately differentiated cancer': 'MD',\n",
        "                        'poorly differentiated cancer': 'PD',\n",
        "                        'benign': 'BN',\n",
        "                        'well differentiated cancer': 'WD'})\n",
        "    parser.add_argument('--custom_order', default=['BN', 'WD', 'MD', 'PD'])\n",
        "\n",
        "    args, unknown = parser.parse_known_args() # Modified to handle unknown arguments\n",
        "\n",
        "    args.mapping = config_for_dataset(args.dataset)[0]\n",
        "    args.custom_order = config_for_dataset(args.dataset)[-1]\n",
        "\n",
        "    # Calculate similarity and return class labels, args\n",
        "    return calculate_sim(args), args"
      ],
      "metadata": {
        "id": "tofwpOuLQ_79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T-SNE"
      ],
      "metadata": {
        "id": "5-zgqbH7umW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dict to store cluster labels\n",
        "cluster_labels = {}\n",
        "for dataset in ['BACH', 'Colon', 'WSSS4LUAD', 'Bladder']:\n",
        "  for postfix in ['_train', '_valid', '_test']:\n",
        "    # run the main function\n",
        "    cluster_labels[postfix], args = main(dataset, postfix)\n",
        "\n",
        "  # perform t-SNE after train, valid, and test pkl have been generated\n",
        "  # first use cluster_labels as first arg to let it know to use cluster labels\n",
        "  plotting_data_split(cluster_labels, args)\n",
        "  # then use empty cluster_labels arg to let it know to use ground truth labels\n",
        "  plotting_data_split({}, args)\n",
        "  # empty out cluster_labels so that we just pass cluster_labels per dataset\n",
        "  cluster_labels = {}"
      ],
      "metadata": {
        "id": "QgS5ct-x2Sv9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensionality"
      ],
      "metadata": {
        "id": "-sa3lDPMELTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each embedding is dimension 512\n",
        "\n",
        "BACH train, test, valid counts:\n",
        "torch.Size([8752, 512]),torch.Size([2674, 512]),torch.Size([2832, 512])\n",
        "\n",
        "\n",
        "Colon train, test, valid counts:\n",
        "torch.Size([7027, 512]),torch.Size([1242, 512]),torch.Size([1588, 512])\n",
        "\n",
        "\n",
        "WSSS4LUAD train, test, valid counts:\n",
        "torch.Size([10091, 512]),torch.Size([1372, 512]),torch.Size([2063, 512])\n",
        "\n",
        "Bladder train, test, valid counts:\n",
        "torch.Size([26450, 512]),torch.Size([12912, 512]),torch.Size([19204, 512])"
      ],
      "metadata": {
        "id": "1o5HiE6x4bYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_feature_path = \"\";\n",
        "# args = argparse.Namespace()\n",
        "# for dataset in ['BACH', 'Colon', 'WSSS4LUAD', 'Bladder']:\n",
        "#   args.dataset = dataset\n",
        "#   train_feature_path = f'results/{args.dataset}/Neoplastic_Process_1000_1000/image_text_representation_train.pkl'\n",
        "#   with open(train_feature_path, 'rb') as f:\n",
        "#       train_feature = pickle.load(f)\n",
        "\n",
        "#   valid_feature_path = f'results/{args.dataset}/Neoplastic_Process_1000_1000/image_text_representation_valid.pkl'\n",
        "#   with open(valid_feature_path, 'rb') as f:\n",
        "#       valid_feature = pickle.load(f)\n",
        "\n",
        "#   test_feature_path = f'results/{args.dataset}/Neoplastic_Process_1000_1000/image_text_representation_test.pkl'\n",
        "#   with open(test_feature_path, 'rb') as f:\n",
        "#       test_feature = pickle.load(f)\n",
        "\n",
        "#   print(f'{args.dataset} train, test, valid counts:')\n",
        "#   print(str(train_feature.shape) + \",\" + str(valid_feature.shape) + \",\" + str(test_feature.shape))"
      ],
      "metadata": {
        "id": "Mcs322RN44Uo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Images"
      ],
      "metadata": {
        "id": "_atiJtGm6w4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# arrays of paths for images\n",
        "ground_truth_label_image_paths = [\n",
        "    'tsne_plot_BACH_true_labels.png',\n",
        "    'tsne_plot_Colon_true_labels.png',\n",
        "    'tsne_plot_WSSS4LUAD_true_labels.png',\n",
        "    'tsne_plot_Bladder_true_labels.png'\n",
        "]\n",
        "\n",
        "cluster_label_image_paths = [\n",
        "    'tsne_plot_BACH_cluster_labels.png',\n",
        "    'tsne_plot_Colon_cluster_labels.png',\n",
        "    'tsne_plot_WSSS4LUAD_cluster_labels.png',\n",
        "    'tsne_plot_Bladder_cluster_labels.png'\n",
        "]\n",
        "\n",
        "# open the images\n",
        "gtl_images = [Image.open(p) for p in ground_truth_label_image_paths]\n",
        "cl_images = [Image.open(p) for p in cluster_label_image_paths]\n",
        "\n",
        "# resize to 6k by 6k\n",
        "w, h = 6000, 6000\n",
        "\n",
        "for i in range(len(gtl_images)):\n",
        "  gtl_images[i] = gtl_images[i].resize((w, h))\n",
        "  cl_images[i] = cl_images[i].resize((w, h))\n",
        "\n",
        "# create blank canvas for all images\n",
        "NUM_COL = 4\n",
        "NUM_ROW = 2\n",
        "grid_w, grid_h = NUM_ROW * w, NUM_COL * h\n",
        "canvas = Image.new('RGB', (grid_w, grid_h))\n",
        "\n",
        "# start with first (cluster label) column\n",
        "for idx, img in enumerate(cl_images):\n",
        "  x = 0\n",
        "  y = idx * h\n",
        "  canvas.paste(img, (x, y))\n",
        "\n",
        "# then do ground truth label column\n",
        "for idx, img in enumerate(gtl_images):\n",
        "  x = w\n",
        "  y = idx * h\n",
        "  canvas.paste(img, (x, y))\n",
        "\n",
        "canvas.save(\"grid.png\")"
      ],
      "metadata": {
        "id": "W07R-dZ-6wKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}